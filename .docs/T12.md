# T12 ‚Äî LLM Prompt Configuration Module

**Goal:**
Centralize LLM prompts, schemas, and retry strategy into a dedicated configuration layer that can be reused across the app.

---

## üçÄ Deliverables

- New config module (e.g., `src/lib/llm-prompts.ts`) exporting typed prompt definitions.
- Updated `idea-generator.ts` (and any other callers) to consume the shared config instead of hardcoded strings.
- Optional unit test or fixture verifying prompt shape.

---

## üõ† Steps

1. **Design prompt types:**
   - Define interfaces describing prompt inputs and expected JSON response structure.
   - Include enums for model selection or response format when relevant.
2. **Extract prompt strings:**
   - Move filter and idea-generation prompts out of `idea-generator.ts` into the new module.
   - Provide helper functions that accept dynamic payloads (e.g., serialized posts) and return final prompt text.
3. **Update callers:**
   - Refactor `filterRelevantPosts` and `generateIdeasFromPosts` to import the helpers instead of embedding multiline strings.
   - Ensure TypeScript infers the response type when parsing JSON.
4. **Document usage:**
   - Add inline JSDoc or comments describing how to extend prompts for future tasks (e.g., scoring tweaks).

---

## üí° Notes

- Keep prompts in plain strings; avoid template engines that could complicate escaping.
- Consider exporting constants for model names to avoid typos.
- Leave room for future prompt variations (e.g., summarization, clustering) within the same module.

